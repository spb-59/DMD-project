{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of ECG signals\n",
    "\n",
    "This document highlights the process of signal processing used in this project. The script used to run the actual processing can be found at scripts/processing.py. This notebook is meant to highlight the process and reasoning behind using certain methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Logistics\n",
    "\n",
    "### 0.1 Loading the files \n",
    "The following cell block loads in all the records from the test directory into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for handling loading in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../physionet.org/files/ecg-arrhythmia/1.0.0/WFDBRecords/01/010\"\n",
    "recordPath = []\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "\n",
    "    files.sort()\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".mat\"):\n",
    "\n",
    "            record_name = os.path.splitext(file)[0]\n",
    "\n",
    "            record_path = os.path.join(root, record_name)\n",
    "\n",
    "\n",
    "            recordPath.append(record_path) # has all record paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Getting the signal data\n",
    "\n",
    "The next step is to get the signal data. For that wfdb is used which is a standard library for looking at ECG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wfdb.io import rdrecord\n",
    "from wfdb import Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "records:list[Record] = []\n",
    "sf = 0\n",
    "for r in recordPath:\n",
    "    record = rdrecord(\n",
    "r\n",
    "    )\n",
    "    if sf == 0:\n",
    "        sf = record.fs\n",
    "    elif sf != record.fs:\n",
    "        raise ValueError(\"Sampling frequency of signals do not match\")\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Writing Processed signal files\n",
    "\n",
    "Once again we use the wfdb library for this task. This can be further reviewed in the script file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4  Extracting comments from the file\n",
    "\n",
    "The conditions names are written with a snomed code so to implement labels for our model a function was made that converts the code to a condition name. Details of this function can be found in the code block bellow. This step is skipped here because it is done when writing to the file can be seen in the script file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SNOMED = pd.read_csv('../physionet-data/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/ConditionNames_SNOMED-CT.csv')\n",
    "\n",
    "\n",
    "mapping = pd.Series(SNOMED['Acronym Name'].values, index=SNOMED['Snomed_CT'].astype(str)).to_dict()\n",
    "\n",
    "\n",
    "def parseConditions(comments):\n",
    "\n",
    "\n",
    "    for data in comments:\n",
    "        if data.startswith(\"Dx:\"):\n",
    "            dx_codes = data.split(\": \")[1].split(\",\")\n",
    "\n",
    "            mapped = [mapping.get(dx, f\"Unknown Dx: {dx}\") for dx in dx_codes]\n",
    "            return mapped\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalisation of signal\n",
    "\n",
    "Before removing noise the first step would be normalise the signal, luckily wfbd has this built in so we just use the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "record=records[0] #first signal for testing, stored as a pandas dataframe\n",
    "fs=record.fs # sampling requency fo record "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb.processing as wd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.p_signal = wd.normalize_bound(record.p_signal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline Wander Removal\n",
    "\n",
    "Second noise removal is the baseline wander removal of the signal. This has been done with a high pass filter. Scipy signal library was used to achieve this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBaselineWander(ecg_signal:pd.DataFrame,sf):\n",
    "    \n",
    "    sampling_rate = sf  \n",
    "    cutoff_frequency = 0.8\n",
    "    nyquist_rate = sampling_rate / 2\n",
    "\n",
    "    index=['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'] \n",
    "\n",
    "    b, a = signal.butter(1, cutoff_frequency / nyquist_rate, btype='highpass')\n",
    "    for i in index:    \n",
    "        ecg_signal[i] = signal.filtfilt(b, a, ecg_signal[i])\n",
    "    return ecg_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordDf=record.to_dataframe()# converting for easier manipulation\n",
    "filtered=removeBaselineWander(recordDf.copy(),fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
